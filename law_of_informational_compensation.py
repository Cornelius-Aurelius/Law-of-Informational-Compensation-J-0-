# -*- coding: utf-8 -*-
"""Law of Informational Compensation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ILBmjJY_ht-eGHwcoL-jEKPPkG9fXMLF
"""

!pip install numpy scipy

import numpy as np
from scipy.stats import entropy

# Sample data representing two distributions (for KL Divergence)
p = np.array([0.1, 0.4, 0.3, 0.2])  # Distribution p
q = np.array([0.25, 0.25, 0.25, 0.25])  # Distribution q (uniform distribution)

# Shannon Entropy Calculation
def shannon_entropy(p):
    return entropy(p, base=2)

# Kullback-Leibler Divergence Calculation
def kl_divergence(p, q):
    return np.sum(p * np.log(p / q))

# Informational Balance Test (δJ ≈ 0)
entropy_p = shannon_entropy(p)
kl_div = kl_divergence(p, q)

print(f"Shannon Entropy of p: {entropy_p}")
print(f"KL Divergence between p and q: {kl_div}")

# Checking for balance (for δJ ≈ 0, divergence should be minimal)
if kl_div < 0.01:
    print("System is in Informational Equilibrium!")
else:
    print("System is not in Equilibrium.")

Shannon Entropy of p: [some value]
KL Divergence between p and q: [some value]
System is in Informational Equilibrium! (or not)

import numpy as np
from scipy.stats import entropy

# Sample data representing two distributions (for KL Divergence)
p = np.array([0.1, 0.4, 0.3, 0.2])  # Distribution p
q = np.array([0.25, 0.25, 0.25, 0.25])  # Distribution q (uniform distribution)

# Shannon Entropy Calculation
def shannon_entropy(p):
    return entropy(p, base=2)

# Kullback-Leibler Divergence Calculation
def kl_divergence(p, q):
    return np.sum(p * np.log(p / q))

# Informational Balance Test (δJ ≈ 0)
entropy_p = shannon_entropy(p)
kl_div = kl_divergence(p, q)

# Printing the values
print(f"Shannon Entropy of p: {entropy_p}")
print(f"KL Divergence between p and q: {kl_div}")

# Checking for balance (for δJ ≈ 0, divergence should be minimal)
if kl_div < 0.01:
    print("System is in Informational Equilibrium!")
else:
    print("System is not in Equilibrium.")

import numpy as np
from scipy.stats import entropy

# Sample data representing two distributions (for KL Divergence)
p = np.array([0.1, 0.4, 0.3, 0.2])  # Distribution p
q = np.array([0.25, 0.25, 0.25, 0.25])  # Distribution q (uniform distribution)

# Shannon Entropy Calculation
def shannon_entropy(p):
    return entropy(p, base=2)

# Kullback-Leibler Divergence Calculation
def kl_divergence(p, q):
    return np.sum(p * np.log(p / q))

# Informational Balance Test (δJ ≈ 0)
entropy_p = shannon_entropy(p)
kl_div = kl_divergence(p, q)

# Correcting print formatting
print(f"Shannon Entropy of p: {entropy_p}")
print(f"KL Divergence between p and q: {kl_div}")

# Checking for balance (for δJ ≈ 0, divergence should be minimal)
if kl_div < 0.01:
    print("System is in Informational Equilibrium!")
else:
    print("System is not in Equilibrium.")

import numpy as np
from scipy.stats import entropy

# More similar distributions (for KL Divergence)
p = np.array([0.25, 0.25, 0.25, 0.25])  # Distribution p (uniform distribution)
q = np.array([0.25, 0.25, 0.25, 0.25])  # Distribution q (uniform distribution)

# Shannon Entropy Calculation
def shannon_entropy(p):
    return entropy(p, base=2)

# Kullback-Leibler Divergence Calculation
def kl_divergence(p, q):
    return np.sum(p * np.log(p / q))

# Informational Balance Test (δJ ≈ 0)
entropy_p = shannon_entropy(p)
kl_div = kl_divergence(p, q)

# Printing the values
print(f"Shannon Entropy of p: {entropy_p}")
print(f"KL Divergence between p and q: {kl_div}")

# Checking for balance (for δJ ≈ 0, divergence should be minimal)
if kl_div < 0.01:
    print("System is in Informational Equilibrium!")
else:
    print("System is not in Equilibrium.")

import numpy as np
from scipy.stats import entropy

# Non-uniform distribution for p and uniform distribution for q
p = np.array([0.1, 0.2, 0.3, 0.4])  # Non-uniform distribution (p)
q = np.array([0.25, 0.25, 0.25, 0.25])  # Uniform distribution (q)

# Shannon Entropy Calculation
def shannon_entropy(p):
    return entropy(p, base=2)

# Kullback-Leibler Divergence Calculation
def kl_divergence(p, q):
    return np.sum(p * np.log(p / q))

# Informational Balance Test (δJ ≈ 0)
entropy_p = shannon_entropy(p)
kl_div = kl_divergence(p, q)

# Printing the values
print(f"Shannon Entropy of p: {entropy_p}")
print(f"KL Divergence between p and q: {kl_div}")

# Checking for balance (for δJ ≈ 0, divergence should be minimal)
if kl_div < 0.01:
    print("System is in Informational Equilibrium!")
else:
    print("System is not in Equilibrium.")

import numpy as np
from scipy.stats import entropy

# Generating a large non-uniform distribution (p) and a uniform distribution (q)
p_large = np.random.rand(1000)  # Large non-uniform random distribution
p_large /= p_large.sum()  # Normalize to sum to 1 (to create a valid probability distribution)
q_large = np.ones(1000) / 1000  # Uniform distribution (each value is 1/1000)

# Shannon Entropy Calculation for large dataset
def shannon_entropy(p):
    return entropy(p, base=2)

# Kullback-Leibler Divergence Calculation for large dataset
def kl_divergence(p, q):
    return np.sum(p * np.log(p / q))

# Informational Balance Test (δJ ≈ 0)
entropy_p_large = shannon_entropy(p_large)
kl_div_large = kl_divergence(p_large, q_large)

# Printing the values
print(f"Shannon Entropy of large p: {entropy_p_large}")
print(f"KL Divergence between large p and q: {kl_div_large}")

# Checking for balance (for δJ ≈ 0, divergence should be minimal)
if kl_div_large < 0.01:
    print("Large system is in Informational Equilibrium!")
else:
    print("Large system is not in Equilibrium.")

